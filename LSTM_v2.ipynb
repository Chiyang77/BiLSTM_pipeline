{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globals  : torch.Size([16, 32])\n",
      "elem_type: torch.Size([16, 508])\n",
      "targets  : torch.Size([16, 508, 2])\n",
      "lengths  : tensor([365, 214, 255, 286, 508, 262, 186, 239, 432, 182, 231, 140, 277, 224,\n",
      "        237, 473])\n"
     ]
    }
   ],
   "source": [
    "# arranger.py\n",
    "# ------------\n",
    "# Build a PyTorch Dataset & DataLoader for jumper‑unity prediction\n",
    "# Author: ChatGPT • May‑2025\n",
    "# -------------------------------------------\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  Set the root folders (EDIT if your paths are different)\n",
    "# ------------------------------------------------------------------\n",
    "ROOT = Path(r\"C:\\Users\\cyang\\Desktop\\Chi\\functions\\Jumper_ML\")\n",
    "INPUT_GLOB   = str(ROOT / \"inputs_*.txt\")\n",
    "\n",
    "# Unity_elements_{batch}_{model}.txt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Small helpers\n",
    "# ------------------------------------------------------------------\n",
    "def read_inputs_file(path: Path) -> np.ndarray:\n",
    "    \"\"\"Return a (N, 32) float array of global inputs *and* the file’s batch‑id.\"\"\"\n",
    "    batch_id = int(path.stem.split(\"_\")[1])          # 'inputs_12.txt' -> 12\n",
    "    status_dir = ROOT / f\"Abaqus_files_{batch_id}\\\\model_status.txt\"\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            rows.append([float(x) for x in parts[0:32]])   # skip model_name\n",
    "    return batch_id, np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "\n",
    "def read_unity_file(path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return (elem_type_ids [L], unity_long [L], unity_comb [L])\n",
    "    \"\"\"\n",
    "    et, ul, uc = [], [], []\n",
    "    with open(path, \"r\") as f:\n",
    "        for row in f:\n",
    "            if not row.strip():\n",
    "                continue\n",
    "            cols = row.split()\n",
    "            # Map element type strings to integer codes\n",
    "            elem_type = 0 if cols[0] == \"PIPE31H\" else 1  # PIPE31H -> 0, ELBOW31 -> 1\n",
    "            et.append(elem_type)\n",
    "            ul.append(float(cols[1]))\n",
    "            uc.append(float(cols[2]))\n",
    "        # print(et)\n",
    "    return (np.asarray(et, dtype=np.int64),\n",
    "            np.asarray(ul, dtype=np.float32),\n",
    "            np.asarray(uc, dtype=np.float32))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Build memory‑light index of every run(model)\n",
    "# ------------------------------------------------------------------\n",
    "class RunIndex:\n",
    "    \"\"\"Maps an integer idx -> (global_input, elem_type[], unity_targets[])\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.globals:   List[np.ndarray] = []\n",
    "        self.elemtypes: List[np.ndarray] = []\n",
    "        self.targets:   List[np.ndarray] = []\n",
    "\n",
    "        for batch_id in range(1,65):\n",
    "            in_path = ROOT/f\"inputs_{batch_id}.txt\"\n",
    "            batch_id, g_matrix = read_inputs_file(Path(in_path))\n",
    "            n_models = g_matrix.shape[0]\n",
    "\n",
    "            for m_idx in range(n_models):\n",
    "                ELEM_DIR     = ROOT / f\"Abaqus_files_{batch_id}\\Step2\\elements\"\n",
    "                unity_path = ELEM_DIR / f\"Unity_elements_{batch_id}_{m_idx+1}.txt\"\n",
    "                if not unity_path.exists():\n",
    "                    continue\n",
    "\n",
    "                et, ul, uc = read_unity_file(unity_path)\n",
    "                tgt = np.stack([ul, uc], axis=1)          # (L, 2)\n",
    "\n",
    "                self.globals.append(g_matrix[m_idx])\n",
    "                self.elemtypes.append(et)\n",
    "                self.targets.append(tgt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.globals)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return (self.globals[idx],     # (29,)\n",
    "                self.elemtypes[idx],   # (L,)\n",
    "                self.targets[idx])     # (L, 2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Convert RunIndex into a torch Dataset\n",
    "# ------------------------------------------------------------------\n",
    "class JumperDataset(Dataset):\n",
    "    def __init__(self, run_index: RunIndex):\n",
    "        self.ri = run_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ri)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g, et, tgt = self.ri[idx]\n",
    "        return (torch.from_numpy(g),\n",
    "                torch.from_numpy(et),\n",
    "                torch.from_numpy(tgt),\n",
    "                torch.tensor(len(et), dtype=torch.long))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  collate_fn  → pads to L_max and builds length tensor\n",
    "# ------------------------------------------------------------------\n",
    "def collate_fn(batch):\n",
    "    g_list, et_list, tgt_list, len_list = zip(*batch)\n",
    "    L_max = max(l.item() for l in len_list)\n",
    "\n",
    "    # Pad element types with 0, targets with 0.0\n",
    "    et_pad = torch.zeros((len(batch), L_max), dtype=torch.long)\n",
    "    tgt_pad = torch.zeros((len(batch), L_max, 2), dtype=torch.float32)\n",
    "\n",
    "    for i, (et, tgt) in enumerate(zip(et_list, tgt_list)):\n",
    "        L = et.shape[0]\n",
    "        et_pad[i, :L] = et\n",
    "        tgt_pad[i, :L, :] = tgt\n",
    "\n",
    "    g_batch   = torch.stack(g_list, dim=0)               # (B, 29)\n",
    "    lengths   = torch.stack(len_list, dim=0)             # (B,)\n",
    "    return g_batch, et_pad, tgt_pad, lengths\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Usage example\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    runs  = RunIndex()\n",
    "    ds    = JumperDataset(runs)\n",
    "    dl    = DataLoader(ds,\n",
    "                       batch_size=16,\n",
    "                       shuffle=True,\n",
    "                       num_workers=0,\n",
    "                       collate_fn=collate_fn)\n",
    "\n",
    "    # quick sanity check\n",
    "    for g, et, tgt, lengths in dl:\n",
    "        print(f\"globals  : {g.shape}\")        # (B, 29)\n",
    "        print(f\"elem_type: {et.shape}\")       # (B, L_max)\n",
    "        print(f\"targets  : {tgt.shape}\")      # (B, L_max, 2)\n",
    "        print(f\"lengths  : {lengths}\")        # (B,)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_in_runindex(run_index: RunIndex, batch_id: int, model_idx: int):\n",
    "    \"\"\"\n",
    "    Find a specific model in the RunIndex by batch_id and model_idx.\n",
    "    Returns (idx, global_inputs, elem_types, targets) if found, or (None, None, None, None) if not found.\n",
    "    \"\"\"\n",
    "    # Read the inputs file to get the global inputs for this batch\n",
    "    try:\n",
    "        in_path = ROOT/f\"inputs_{batch_id}.txt\"\n",
    "        _, g_matrix = read_inputs_file(Path(in_path))\n",
    "        target_global = g_matrix[model_idx-1]  # model_idx is 1-based in filenames\n",
    "        \n",
    "        # Search through RunIndex to find matching global inputs\n",
    "        for idx in range(len(run_index)):\n",
    "            global_inputs, elem_types, targets = run_index[idx]\n",
    "            # Check if this is the model we're looking for by comparing global inputs\n",
    "            if np.array_equal(global_inputs, target_global):\n",
    "                return idx, global_inputs, elem_types, targets\n",
    "        \n",
    "        # If we get here, the model wasn't found\n",
    "        print(f\"Model with batch_id={batch_id}, model_idx={model_idx} not found in RunIndex\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globals  : torch.Size([16, 32])\n",
      "elem_type: torch.Size([16, 467])\n",
      "targets  : torch.Size([16, 467, 2])\n",
      "lengths  : tensor([218, 231, 210, 272, 177, 385, 266, 172, 467, 199, 266, 165, 284, 204,\n",
      "        340, 361])\n",
      "\n",
      "As tensors (what would go into the model):\n",
      "  Global tensor: tensor([ 4.1598e+01,  1.5302e+01,  4.1554e+01,  6.1456e+01,  4.1553e+01,\n",
      "         1.5302e+01,  5.1720e+01,  5.6464e+01,  1.6450e+02,  5.0576e+03,\n",
      "         4.7247e+05,  1.1458e-01,  7.1875e-01,  1.9000e+01, -1.2500e-01,\n",
      "        -1.2500e-01,  1.2500e-01,  1.7450e-02, -8.7300e-03,  0.0000e+00,\n",
      "         1.2500e-01, -1.2500e-01,  1.2500e-01,  8.7300e-03, -8.7300e-03,\n",
      "         0.0000e+00, -2.0786e+00, -1.0772e+00, -4.2591e+00,  1.2198e+00,\n",
      "        -9.2955e-01, -1.4098e+00])\n",
      "  Element types tensor: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])...\n",
      "  Targets tensor: tensor([[0.3895, 0.4063],\n",
      "        [0.3916, 0.4082],\n",
      "        [0.3937, 0.4102],\n",
      "        [0.3957, 0.4121],\n",
      "        [0.3976, 0.4139]])\n",
      "  Length: 362\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Original sanity check\n",
    "    runs = RunIndex()\n",
    "    ds = JumperDataset(runs)\n",
    "    dl = DataLoader(ds,\n",
    "                   batch_size=16,\n",
    "                   shuffle=True,\n",
    "                   num_workers=0,\n",
    "                   collate_fn=collate_fn)\n",
    "\n",
    "    # Quick sanity check\n",
    "    for g, et, tgt, lengths in dl:\n",
    "        print(f\"globals  : {g.shape}\")        # (B, 29)\n",
    "        print(f\"elem_type: {et.shape}\")       # (B, L_max)\n",
    "        print(f\"targets  : {tgt.shape}\")      # (B, L_max, 2)\n",
    "        print(f\"lengths  : {lengths}\")        # (B,)\n",
    "        break\n",
    "    \n",
    "    # Find a specific model in the RunIndex\n",
    "    # Change these values to inspect different models\n",
    "    idx, g, et, tgt = find_model_in_runindex(runs, batch_id=4, model_idx=1)\n",
    "    \n",
    "    if idx is not None:\n",
    "        # Convert to tensors to verify the exact data that would go into the model\n",
    "        g_tensor = torch.from_numpy(g)\n",
    "        et_tensor = torch.from_numpy(et)\n",
    "        tgt_tensor = torch.from_numpy(tgt)\n",
    "        length_tensor = torch.tensor(len(et), dtype=torch.long)\n",
    "        \n",
    "        print(\"\\nAs tensors (what would go into the model):\")\n",
    "        print(f\"  Global tensor: {g_tensor}\")\n",
    "        print(f\"  Element types tensor: {et_tensor[:10]}...\")\n",
    "        print(f\"  Targets tensor: {tgt_tensor[:5]}\")\n",
    "        print(f\"  Length: {length_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 01   train-loss: 0.2347\n",
      "Epoch 02   train-loss: 0.1983\n",
      "Epoch 03   train-loss: 0.1905\n",
      "Epoch 04   train-loss: 0.1883\n",
      "Epoch 05   train-loss: 0.1860\n",
      "Epoch 06   train-loss: 0.1852\n",
      "Epoch 07   train-loss: 0.1828\n",
      "Epoch 08   train-loss: 0.1778\n",
      "Epoch 09   train-loss: 0.1770\n",
      "Epoch 10   train-loss: 0.1770\n",
      "Epoch 11   train-loss: 0.1757\n",
      "Epoch 12   train-loss: 0.1762\n",
      "Epoch 13   train-loss: 0.1755\n",
      "Epoch 14   train-loss: 0.1726\n",
      "Epoch 15   train-loss: 0.1696\n",
      "Epoch 16   train-loss: 0.1693\n",
      "Epoch 17   train-loss: 0.1685\n",
      "Epoch 18   train-loss: 0.1686\n",
      "Epoch 19   train-loss: 0.1682\n",
      "Epoch 20   train-loss: 0.1679\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# train_unity.py\n",
    "# ---------------\n",
    "# Train a bi‑LSTM to predict (unity_long, unity_comb) per element\n",
    "# ---------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE }\")\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Hyper‑parameters – tweak as you like\n",
    "# ----------------------------------------------------------------------\n",
    "BATCH_SIZE      = 32\n",
    "EMBED_DIM       = 8        # element‑type embedding size\n",
    "HIDDEN_DIM      = 64       # per‑direction LSTM hidden\n",
    "NUM_LAYERS      = 2\n",
    "DROPOUT         = 0.2\n",
    "LR              = 3e-4\n",
    "EPOCHS          = 20\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Model definition\n",
    "# ----------------------------------------------------------------------\n",
    "class UnityBiLSTM(nn.Module):\n",
    "    def __init__(self, n_elem_types: int, n_globals: int = 32):\n",
    "        super().__init__()\n",
    "        self.elem_embed = nn.Embedding(n_elem_types, EMBED_DIM, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=EMBED_DIM + n_globals,\n",
    "            hidden_size=HIDDEN_DIM,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)           # outputs: unity_long, unity_comb\n",
    "        )\n",
    "\n",
    "    def forward(self, g, et, lengths):\n",
    "        \"\"\"\n",
    "        g       : (B, 32)         global scalars\n",
    "        et      : (B, L_max)      element‑type IDs (0=PAD)\n",
    "        lengths : (B,)            real sequence lengths\n",
    "        \"\"\"\n",
    "        B, L = et.size()\n",
    "\n",
    "        # 1) embed element types\n",
    "        et_emb = self.elem_embed(et)                  # (B, L, EMBED_DIM)\n",
    "\n",
    "        # 2) repeat globals along time axis\n",
    "        g_rep = g.unsqueeze(1).expand(-1, L, -1)      # (B, L, 32)\n",
    "\n",
    "        # 3) concat & pack\n",
    "        x = torch.cat([et_emb, g_rep], dim=-1)        # (B, L, D_in)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # 4) bi‑LSTM\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        h, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True, total_length=L\n",
    "        )                                             # (B, L, 2*HIDDEN_DIM)\n",
    "\n",
    "        # 5) per‑time‑step head\n",
    "        y_hat = self.head(h)                          # (B, L, 2)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Utilities – masked MSE so PAD steps don’t count\n",
    "# ----------------------------------------------------------------------\n",
    "def masked_mse(pred, target, lengths):\n",
    "    \"\"\"\n",
    "    pred, target : (B, L_max, 2)\n",
    "    lengths      : (B,)\n",
    "    \"\"\"\n",
    "    B, L, _ = pred.shape\n",
    "    mask = torch.arange(L, device=lengths.device).expand(B, L) < lengths.unsqueeze(1)\n",
    "    mask = mask.unsqueeze(-1)                        # (B, L, 1)\n",
    "\n",
    "    diff2 = (pred - target) ** 2 * mask\n",
    "    mse = diff2.sum() / mask.sum()                  # mean over real steps\n",
    "    return mse\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Data\n",
    "# ----------------------------------------------------------------------\n",
    "runs = RunIndex()\n",
    "ds   = JumperDataset(runs)\n",
    "dl   = DataLoader(ds,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True,\n",
    "                  num_workers=0,\n",
    "                  collate_fn=collate_fn,\n",
    "                  pin_memory=True)\n",
    "\n",
    "# how many element‑type IDs? 0 = PAD plus real ones (here 0/1 → 2)\n",
    "N_ELEM_TYPES = max([et.max() for et in runs.elemtypes]) + 1\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5.  Train loop\n",
    "# ----------------------------------------------------------------------\n",
    "model = UnityBiLSTM(n_elem_types=N_ELEM_TYPES, n_globals=32).to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for g, et, tgt, lengths in dl:\n",
    "        g, et, tgt, lengths = g.to(DEVICE), et.to(DEVICE), tgt.to(DEVICE), lengths.to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(g, et, lengths)\n",
    "        loss  = masked_mse(y_hat, tgt, lengths)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}   train-loss: {running / len(dl):.4f}\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to unity_bilstm.pt\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"unity_bilstm.pt\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 151 elements (OD=1.333, total pipe length=213.0m)\n",
      "\n",
      " idx │ elem_type │ unity_long │ unity_comb\n",
      "─────┼───────────┼────────────┼────────────\n",
      "   0 │   PIPE    │     0.7816 │     0.9047\n",
      "   1 │   PIPE    │     0.7682 │     0.8873\n",
      "   2 │   PIPE    │     0.7427 │     0.8621\n",
      "   3 │   PIPE    │     0.7117 │     0.8310\n",
      "   4 │   PIPE    │     0.6842 │     0.8034\n",
      "   5 │   PIPE    │     0.6602 │     0.7822\n",
      "   6 │   PIPE    │     0.6413 │     0.7655\n",
      "   7 │   PIPE    │     0.6309 │     0.7565\n",
      "   8 │   PIPE    │     0.6249 │     0.7512\n",
      "   9 │   PIPE    │     0.6217 │     0.7484\n",
      "  10 │   PIPE    │     0.6277 │     0.7536\n",
      "  11 │   PIPE    │     0.6514 │     0.7739\n",
      "  12 │   PIPE    │     0.6803 │     0.8009\n",
      "  13 │   PIPE    │     0.7087 │     0.8270\n",
      "  14 │   PIPE    │     0.7372 │     0.8528\n",
      "  15 │   PIPE    │     0.7668 │     0.8793\n",
      "  16 │   PIPE    │     0.7984 │     0.9087\n",
      "  17 │   ELBOW   │     0.8321 │     0.9399\n",
      "  18 │   ELBOW   │     0.8663 │     0.9714\n",
      "  19 │   ELBOW   │     0.8978 │     0.9999\n",
      "  20 │   ELBOW   │     0.9219 │     1.0224\n",
      "  21 │   ELBOW   │     0.9360 │     1.0365\n",
      "  22 │   ELBOW   │     0.9394 │     1.0405\n",
      "  23 │   ELBOW   │     0.9334 │     1.0353\n",
      "  24 │   ELBOW   │     0.9187 │     1.0231\n",
      "  25 │   PIPE    │     0.8965 │     1.0048\n",
      "  26 │   ELBOW   │     0.8685 │     0.9810\n",
      "  27 │   ELBOW   │     0.8360 │     0.9526\n",
      "  28 │   ELBOW   │     0.8000 │     0.9205\n",
      "  29 │   ELBOW   │     0.7674 │     0.8918\n",
      "  30 │   ELBOW   │     0.7445 │     0.8731\n",
      "  31 │   ELBOW   │     0.7210 │     0.8533\n",
      "  32 │   ELBOW   │     0.6981 │     0.8332\n",
      "  33 │   ELBOW   │     0.6831 │     0.8187\n",
      "  34 │   PIPE    │     0.6701 │     0.8053\n",
      "  35 │   PIPE    │     0.6580 │     0.7926\n",
      "  36 │   PIPE    │     0.6467 │     0.7806\n",
      "  37 │   PIPE    │     0.6363 │     0.7692\n",
      "  38 │   PIPE    │     0.6266 │     0.7584\n",
      "  39 │   PIPE    │     0.6174 │     0.7482\n",
      "  40 │   ELBOW   │     0.6089 │     0.7384\n",
      "  41 │   ELBOW   │     0.6009 │     0.7297\n",
      "  42 │   ELBOW   │     0.5937 │     0.7231\n",
      "  43 │   ELBOW   │     0.5896 │     0.7188\n",
      "  44 │   ELBOW   │     0.5862 │     0.7151\n",
      "  45 │   ELBOW   │     0.5836 │     0.7120\n",
      "  46 │   ELBOW   │     0.5817 │     0.7095\n",
      "  47 │   ELBOW   │     0.5808 │     0.7077\n",
      "  48 │   PIPE    │     0.5809 │     0.7067\n",
      "  49 │   PIPE    │     0.5821 │     0.7065\n",
      "  50 │   PIPE    │     0.5845 │     0.7071\n",
      "  51 │   PIPE    │     0.5881 │     0.7086\n",
      "  52 │   PIPE    │     0.5928 │     0.7108\n",
      "  53 │   PIPE    │     0.5983 │     0.7136\n",
      "  54 │   PIPE    │     0.6044 │     0.7168\n",
      "  55 │   PIPE    │     0.6106 │     0.7201\n",
      "  56 │   PIPE    │     0.6168 │     0.7233\n",
      "  57 │   PIPE    │     0.6225 │     0.7262\n",
      "  58 │   PIPE    │     0.6276 │     0.7286\n",
      "  59 │   PIPE    │     0.6321 │     0.7306\n",
      "  60 │   PIPE    │     0.6360 │     0.7321\n",
      "  61 │   PIPE    │     0.6396 │     0.7347\n",
      "  62 │   PIPE    │     0.6429 │     0.7370\n",
      "  63 │   PIPE    │     0.6458 │     0.7391\n",
      "  64 │   PIPE    │     0.6485 │     0.7410\n",
      "  65 │   PIPE    │     0.6508 │     0.7426\n",
      "  66 │   PIPE    │     0.6529 │     0.7440\n",
      "  67 │   PIPE    │     0.6548 │     0.7451\n",
      "  68 │   PIPE    │     0.6531 │     0.7447\n",
      "  69 │   PIPE    │     0.6512 │     0.7441\n",
      "  70 │   PIPE    │     0.6493 │     0.7434\n",
      "  71 │   PIPE    │     0.6474 │     0.7426\n",
      "  72 │   PIPE    │     0.6456 │     0.7418\n",
      "  73 │   PIPE    │     0.6438 │     0.7409\n",
      "  74 │   PIPE    │     0.6421 │     0.7401\n",
      "  75 │   PIPE    │     0.6405 │     0.7392\n",
      "  76 │   PIPE    │     0.6383 │     0.7381\n",
      "  77 │   PIPE    │     0.6347 │     0.7361\n",
      "  78 │   PIPE    │     0.6312 │     0.7343\n",
      "  79 │   PIPE    │     0.6278 │     0.7325\n",
      "  80 │   PIPE    │     0.6246 │     0.7309\n",
      "  81 │   PIPE    │     0.6215 │     0.7294\n",
      "  82 │   PIPE    │     0.6185 │     0.7280\n",
      "  83 │   PIPE    │     0.6157 │     0.7268\n",
      "  84 │   PIPE    │     0.6130 │     0.7257\n",
      "  85 │   PIPE    │     0.6104 │     0.7247\n",
      "  86 │   PIPE    │     0.6079 │     0.7239\n",
      "  87 │   PIPE    │     0.6055 │     0.7231\n",
      "  88 │   PIPE    │     0.6031 │     0.7225\n",
      "  89 │   PIPE    │     0.6009 │     0.7220\n",
      "  90 │   PIPE    │     0.5987 │     0.7215\n",
      "  91 │   PIPE    │     0.5965 │     0.7210\n",
      "  92 │   ELBOW   │     0.5944 │     0.7204\n",
      "  93 │   ELBOW   │     0.5922 │     0.7199\n",
      "  94 │   ELBOW   │     0.5901 │     0.7192\n",
      "  95 │   ELBOW   │     0.5880 │     0.7183\n",
      "  96 │   ELBOW   │     0.5860 │     0.7174\n",
      "  97 │   ELBOW   │     0.5842 │     0.7165\n",
      "  98 │   ELBOW   │     0.5826 │     0.7154\n",
      "  99 │   ELBOW   │     0.5810 │     0.7140\n",
      " 100 │   PIPE    │     0.5795 │     0.7123\n",
      " 101 │   PIPE    │     0.5810 │     0.7139\n",
      " 102 │   PIPE    │     0.5849 │     0.7181\n",
      " 103 │   PIPE    │     0.5903 │     0.7238\n",
      " 104 │   PIPE    │     0.5975 │     0.7315\n",
      " 105 │   ELBOW   │     0.6055 │     0.7402\n",
      " 106 │   ELBOW   │     0.6142 │     0.7494\n",
      " 107 │   ELBOW   │     0.6224 │     0.7580\n",
      " 108 │   ELBOW   │     0.6293 │     0.7653\n",
      " 109 │   ELBOW   │     0.6353 │     0.7716\n",
      " 110 │   ELBOW   │     0.6409 │     0.7773\n",
      " 111 │   ELBOW   │     0.6468 │     0.7829\n",
      " 112 │   ELBOW   │     0.6536 │     0.7890\n",
      " 113 │   PIPE    │     0.6619 │     0.7962\n",
      " 114 │   ELBOW   │     0.6725 │     0.8047\n",
      " 115 │   ELBOW   │     0.6884 │     0.8170\n",
      " 116 │   ELBOW   │     0.7089 │     0.8325\n",
      " 117 │   ELBOW   │     0.7318 │     0.8495\n",
      " 118 │   ELBOW   │     0.7561 │     0.8673\n",
      " 119 │   ELBOW   │     0.7811 │     0.8853\n",
      " 120 │   ELBOW   │     0.8078 │     0.9043\n",
      " 121 │   ELBOW   │     0.8358 │     0.9243\n",
      " 122 │   PIPE    │     0.8578 │     0.9408\n",
      " 123 │   PIPE    │     0.8680 │     0.9504\n",
      " 124 │   PIPE    │     0.8743 │     0.9577\n",
      " 125 │   PIPE    │     0.8733 │     0.9591\n",
      " 126 │   PIPE    │     0.8612 │     0.9509\n",
      " 127 │   PIPE    │     0.8466 │     0.9407\n",
      " 128 │   PIPE    │     0.8294 │     0.9280\n",
      " 129 │   PIPE    │     0.8093 │     0.9125\n",
      " 130 │   PIPE    │     0.7859 │     0.8938\n",
      " 131 │   PIPE    │     0.7588 │     0.8711\n",
      " 132 │   PIPE    │     0.7273 │     0.8439\n",
      " 133 │   PIPE    │     0.6913 │     0.8119\n",
      " 134 │   PIPE    │     0.6566 │     0.7807\n",
      " 135 │   PIPE    │     0.6488 │     0.7741\n",
      " 136 │   PIPE    │     0.6402 │     0.7660\n",
      " 137 │   PIPE    │     0.6279 │     0.7544\n",
      " 138 │   PIPE    │     0.6138 │     0.7416\n",
      " 139 │   PIPE    │     0.5996 │     0.7289\n",
      " 140 │   PIPE    │     0.5934 │     0.7211\n",
      " 141 │   PIPE    │     0.5947 │     0.7197\n",
      " 142 │   PIPE    │     0.6023 │     0.7243\n",
      " 143 │   PIPE    │     0.6119 │     0.7308\n",
      " 144 │   PIPE    │     0.6231 │     0.7392\n",
      " 145 │   PIPE    │     0.6369 │     0.7499\n",
      " 146 │   PIPE    │     0.6536 │     0.7615\n",
      " 147 │   PIPE    │     0.6952 │     0.7984\n",
      " 148 │   PIPE    │     0.7590 │     0.8594\n",
      " 149 │   PIPE    │     0.8212 │     0.9137\n",
      " 150 │   PIPE    │     0.8569 │     0.9350\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "PIPE, ELBOW = 0, 1  # consistent with training labels\n",
    "\n",
    "def build_elem_types(seg_lengths: List[float],\n",
    "                     outer_diameter: float,\n",
    "                     bend_angle_rad: float = np.pi / 2,\n",
    "                     bend_radius_factor: float = 5.0,\n",
    "                     min_pipe_elements: int = 1) -> np.ndarray:\n",
    "    H = outer_diameter\n",
    "    R = bend_radius_factor * outer_diameter\n",
    "    L_bend = R * bend_angle_rad\n",
    "    n_elbow = max(1, int(np.ceil(L_bend / H)))\n",
    "    actual_L_bend = n_elbow * H  # actual length covered by elbow elements\n",
    "\n",
    "    types = []\n",
    "    num_segments = len(seg_lengths)\n",
    "\n",
    "    for k, Lseg in enumerate(seg_lengths):\n",
    "        num_bends = 1/2 if (k == 0 or k == num_segments - 1) else 1\n",
    "\n",
    "        available_pipe_length = Lseg - num_bends * 10*H\n",
    "\n",
    "        # If too short, force a minimal straight section of at least 1 pipe element\n",
    "        if available_pipe_length < H * min_pipe_elements:\n",
    "            n_pipe = min_pipe_elements\n",
    "        else:\n",
    "            n_pipe = int(np.ceil(available_pipe_length / H))\n",
    "\n",
    "        types.extend([PIPE] * n_pipe)\n",
    "\n",
    "        # Add elbow elements after each segment except the last\n",
    "        if k != num_segments - 1:\n",
    "            types.extend([ELBOW] * n_elbow)\n",
    "\n",
    "    return np.asarray(types, dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_FILE = \"unity_bilstm.pt\"\n",
    "\n",
    "g_vec = np.array([28.89045, 14.46195, 20.23866, 70.89006, 19.33562, 14.46195,\n",
    "                  44.69494, 43.41846, 170.3305, 2623.41446, 1061784.706, 0.10417,\n",
    "                    1.33333, 19, -0.125, 0.125, 0.125, 0.00873, 0.01745, 0, 0.125,\n",
    "                      0.125, -0.125, -0.00873, -0.01745, 0, 3.28873, 1.4538, 3.15864,\n",
    "                        -2.18767, 0.44241, -1.93973], dtype=np.float32)\n",
    "\n",
    "assert g_vec.size == 32\n",
    "\n",
    "seg_len = g_vec[:7]\n",
    "outer_d = g_vec[12]\n",
    "\n",
    "elem_type = build_elem_types(seg_len, outer_d)\n",
    "L = elem_type.size\n",
    "\n",
    "runs = RunIndex()\n",
    "n_typ = max(et.max() for et in runs.elemtypes) + 1\n",
    "model = UnityBiLSTM(n_typ, n_globals=32).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "g_tensor = torch.from_numpy(g_vec).unsqueeze(0).to(DEVICE)\n",
    "et_tensor = torch.from_numpy(elem_type).unsqueeze(0).to(DEVICE)\n",
    "lengths = torch.tensor([L], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(g_tensor, et_tensor, lengths)\n",
    "\n",
    "pred = y_hat[0, :L].cpu().numpy()\n",
    "unity_long, unity_comb = pred[:, 0], pred[:, 1]\n",
    "\n",
    "print(f\"\\nGenerated {L} elements (OD={outer_d:.3f}, total pipe length={seg_len.sum():.1f}m)\\n\")\n",
    "print(\" idx │ elem_type │ unity_long │ unity_comb\")\n",
    "print(\"─────┼───────────┼────────────┼────────────\")\n",
    "for i in range(L):\n",
    "    et_label = \"PIPE\" if elem_type[i] == 0 else \"ELBOW\"\n",
    "    print(f\"{i:4d} │ {et_label:^9} │ {unity_long[i]:10.4f} │ {unity_comb[i]:10.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
